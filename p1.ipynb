{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "from spacy.symbols import nsubj, VERB\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_essay = \"No, i don’t agree with the best way to travel is in a group led. I think in this way they will have many probelme. Firt of all, the group led will be not agree together each one want be the led. Second, when they travel they will be fighting all the time. also, they will not listine to each. n the other hand, when you travel with a group wich has one led, they will be better than onather way for severl reasons. First, all the travels will be nice and specifictly . Next, many people like travel with agroup by one led. Finally, i don’t agree.\"\n",
    "high_essay = \"I would really prefer to travel on my own with plenty on time, but who wouldn’t? Unfor- tunately that is not always possible. It is always nicer to walk looking around at the same time, steping by little shops and cafes, talking to people, asking for directions, going to the places you choose to go to and discovering everything on your own. I think that is the travel ideal for many of us, but we usually have a hard time on finding the time to do it that way, and instead make plans with too many destinations all at once in a small schedule.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Length of the essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_length(essay):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(essay)\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    average_length_high = 20  # 假设高分作文平均句子数\n",
    "    average_length_low = 10   # 假设低分作文平均句子数\n",
    "\n",
    "    # 根据句子数量相对于高低平均值的位置分配得分\n",
    "    if num_sentences < average_length_low:\n",
    "        return 1\n",
    "    elif num_sentences < average_length_high:\n",
    "        return (num_sentences - average_length_low) / (average_length_high - average_length_low) * 4 + 1\n",
    "    else:\n",
    "        return 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_spelling(essay):\n",
    "    spell = SpellChecker()\n",
    "    # 分词并检查拼写\n",
    "    misspelled = spell.unknown(essay.split())\n",
    "    num_errors = len(misspelled)\n",
    "    # 以拼写错误的数量来确定得分，错误越多得分越高，这通常不是期望的评分逻辑\n",
    "    score = min(num_errors*0.1, 4)\n",
    "    return round(score, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3\n",
      "2.3\n"
     ]
    }
   ],
   "source": [
    "high = score_spelling(high_essay)\n",
    "low = score_spelling(low_essay)\n",
    "print(high)\n",
    "print(low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Syntax/Grammar\n",
    "    \n",
    "    (i) Subject-Verb agreement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主谓一致性评分函数\n",
    "def score_subject_verb_agreement(essay):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(essay)\n",
    "    agreement_errors = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos == VERB:\n",
    "            subjects = [child for child in token.children if child.dep == nsubj]\n",
    "            for subject in subjects:\n",
    "                # 单数主语应该对应单数动词形式\n",
    "                if subject.tag_ in ['NN', 'NNP'] and token.tag_ != 'VBZ':\n",
    "                    agreement_errors += 1\n",
    "                # 复数主语应该对应复数动词形式\n",
    "                elif subject.tag_ in ['NNS', 'NNPS'] and token.tag_ != 'VBP':\n",
    "                    agreement_errors += 1\n",
    "\n",
    "    # 根据作文中总句子数量和错误数来计算得分，得分范围在1到5之间\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    # 如果有错误，每个错误会根据句子总数减少得分，但分数至少为1\n",
    "    score = max(5 - (agreement_errors * 5 / num_sentences if num_sentences > 0 else 0), 1)\n",
    "    return round(score, 2)  # 保留两位小数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low essay subject-verb agreement score: 3.89\n",
      "High essay subject-verb agreement score: 5.0\n"
     ]
    }
   ],
   "source": [
    "# 测试低分和高分作文\n",
    "low_essay_score = score_subject_verb_agreement(low_essay)\n",
    "high_essay_score = score_subject_verb_agreement(high_essay)\n",
    "\n",
    "print(f\"Low essay subject-verb agreement score: {low_essay_score}\")\n",
    "print(f\"High essay subject-verb agreement score: {high_essay_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (ii) Verb tense / missing verb / extra verb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动词时态和使用评分函数\n",
    "def score_verb_usage(essay):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(essay)\n",
    "    verb_errors = 0\n",
    "    \n",
    "    for sentence in doc.sents:\n",
    "        has_aux = False\n",
    "        has_main_verb = False\n",
    "        for token in sentence:\n",
    "            # 检测助动词\n",
    "            if token.tag_ == \"MD\":\n",
    "                has_aux = True\n",
    "            # 检测主要动词\n",
    "            if token.pos_ == \"VERB\" and token.dep_ not in [\"aux\", \"auxpass\"]:\n",
    "                has_main_verb = True\n",
    "\n",
    "            # 如果存在助动词，但没有主要动词或另一个助动词，则计为错误\n",
    "            if has_aux and not has_main_verb and token.pos_ != \"VERB\":\n",
    "                verb_errors += 1\n",
    "\n",
    "            # 如果句子结束但缺少主要动词，也计为错误\n",
    "            if token == sentence[-1] and not has_main_verb:\n",
    "                verb_errors += 1\n",
    "\n",
    "            # 这里可以添加更多的检测逻辑，例如检测时态错误等\n",
    "\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    # 错误转换为得分，错误越多得分越低\n",
    "    score = max(5 - (verb_errors * 2 / num_sentences if num_sentences > 0 else 0), 1)\n",
    "    return round(score, 2)  # 保留两位小数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low essay verb usage score: 3.0\n",
      "High essay verb usage score: 3.5\n"
     ]
    }
   ],
   "source": [
    "low_essay_score = score_verb_usage(low_essay)\n",
    "high_essay_score = score_verb_usage(high_essay)\n",
    "\n",
    "print(f\"Low essay verb usage score: {low_essay_score}\")\n",
    "print(f\"High essay verb usage score: {high_essay_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (iii) Sentence formation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence_formation(essay):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(essay)\n",
    "    total_sentences = len(list(doc.sents))\n",
    "    sentence_errors = 0\n",
    "    \n",
    "    for sentence in doc.sents:\n",
    "        has_nsubj = False\n",
    "        has_dobj = False\n",
    "        for token in sentence:\n",
    "            # 检查主语\n",
    "            if token.dep_ == \"nsubj\":\n",
    "                has_nsubj = True\n",
    "            # 检查宾语\n",
    "            if token.dep_ == \"dobj\":\n",
    "                has_dobj = True\n",
    "        \n",
    "        # 如果句子缺少主语或宾语，计为错误\n",
    "        if not has_nsubj or not has_dobj:\n",
    "            sentence_errors += 1\n",
    "\n",
    "        # 这里可以添加更多的检查，例如句子是否以适当的标点结尾\n",
    "\n",
    "    # 计算得分，错误越多得分越低\n",
    "    score = max(5 - (sentence_errors * 5 / total_sentences if total_sentences > 0 else 0), 1)\n",
    "    return round(score, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low essay sentence formation score: 1\n",
      "High essay sentence formation score: 2.5\n"
     ]
    }
   ],
   "source": [
    "low_essay_score = score_sentence_formation(low_essay)\n",
    "high_essay_score = score_sentence_formation(high_essay)\n",
    "\n",
    "print(f\"Low essay sentence formation score: {low_essay_score}\")\n",
    "print(f\"High essay sentence formation score: {high_essay_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Semantics (meaning) / Pragmatics (quality at the paragraph/document level):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (i) Does the essay answer the question / address the topic? we can use word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_semantic_relevance(essay, topic):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")  # 加载包含词向量的大模型\n",
    "    doc = nlp(essay)\n",
    "    topic_doc = nlp(topic)\n",
    "    \n",
    "    # 计算作文和主题之间的相似性\n",
    "    similarity = doc.similarity(topic_doc)\n",
    "    \n",
    "    # 将相似性得分转换为1到5的评分\n",
    "    score = 1 + 4 * similarity  # 假设相似性是0到1之间，将其映射到1到5的评分\n",
    "    return round(score, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low essay semantic relevance score: 3.97\n",
      "High essay semantic relevance score: 3.93\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "topic = \"The best way to travel is in a group led by a tour guide\"  # 主题\n",
    "low_essay_score = score_semantic_relevance(low_essay, topic)\n",
    "high_essay_score = score_semantic_relevance(high_essay, topic)\n",
    "\n",
    "print(f\"Low essay semantic relevance score: {low_essay_score}\")\n",
    "print(f\"High essay semantic relevance score: {high_essay_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (ii) Is the essay coherent? We will use a simple algorithm for reference resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_coherence(essay):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(essay)\n",
    "    coherence_issues = 0\n",
    "    transition_words = {'however', 'therefore', 'furthermore', 'consequently', 'then', 'next', 'moreover', 'likewise', 'additionally', 'finally'}\n",
    "    transition_counts = Counter()\n",
    "\n",
    "    # 遍历每个句子\n",
    "    for sentence in doc.sents:\n",
    "        # 遍历句子中的每个代词\n",
    "        for token in sentence:\n",
    "            if token.pos_ == 'PRON':\n",
    "                # 简化的检查：如果代词的前一个词不是名词或专有名词，可能存在连贯性问题\n",
    "                prev_token = doc[token.i - 1] if token.i > 0 else None\n",
    "                if not (prev_token and prev_token.pos_ in ['NOUN', 'PROPN']):\n",
    "                    coherence_issues += 1\n",
    "        \n",
    "        # 统计转折词的使用情况\n",
    "        transition_counts.update(token.text.lower() for token in sentence if token.text.lower() in transition_words)\n",
    "\n",
    "    # 如果转折词使用次数太少或太多，视为连贯性问题\n",
    "    if transition_counts and (max(transition_counts.values()) < 2 or sum(transition_counts.values()) > len(list(doc.sents)) / 2):\n",
    "        coherence_issues += 1\n",
    "\n",
    "    total_sentences = len(list(doc.sents))\n",
    "    # 计算得分，问题越多得分越低\n",
    "    score = max(5 - (coherence_issues * 5 / total_sentences if total_sentences > 0 else 0), 1)\n",
    "    return round(score, 2)  # 保留两位小数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low essay coherence score: 1\n",
      "High essay coherence score: 1\n"
     ]
    }
   ],
   "source": [
    "low_essay_score = score_coherence(low_essay)\n",
    "high_essay_score = score_coherence(high_essay)\n",
    "\n",
    "print(f\"Low essay coherence score: {low_essay_score}\")\n",
    "print(f\"High essay coherence score: {high_essay_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## total score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_final_score(essay, topic):\n",
    "    # 假设以下函数返回1到5的得分\n",
    "    a = score_length(essay)\n",
    "    b = score_spelling(essay)  # 这个是0到4的得分，我们可能需要转换为1到5的等级\n",
    "    ci = score_subject_verb_agreement(essay)\n",
    "    cii = score_verb_usage(essay)  # 我们假设ci和cii是分开的函数，您可能需要相应调整\n",
    "    ciii = score_sentence_formation(essay)\n",
    "    di = score_semantic_relevance(essay, topic)\n",
    "    dii = score_coherence(essay)  # 假设您已经有一个函数来评估作文的连贯性，这里暂时用1代替\n",
    "\n",
    "    # 打印每个标准的得分\n",
    "    print(f\"Length score (a): {a}\")\n",
    "    print(f\"Spelling score (b): {b}\")\n",
    "    print(f\"Subject-Verb agreement score (c.i): {ci}\")\n",
    "    print(f\"Verb tense usage score (c.ii): {cii}\")\n",
    "    print(f\"Sentence formation score (c.iii): {ciii}\")\n",
    "    print(f\"Semantic relevance score (d.i): {di}\")\n",
    "    print(f\"Coherence score (d.ii): {dii}\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # 按照给定的公式计算最终得分\n",
    "    final_score = 2 * a - b + ci + cii + 2 * ciii + 3 * di + 2 * dii\n",
    "    \n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length score (a): 1\n",
      "Spelling score (b): 2.3\n",
      "Subject-Verb agreement score (c.i): 3.89\n",
      "Verb tense usage score (c.ii): 3.0\n",
      "Sentence formation score (c.iii): 1\n",
      "Semantic relevance score (d.i): 3.97\n",
      "Coherence score (d.ii): 1\n",
      "\n",
      "Length score (a): 1\n",
      "Spelling score (b): 1.3\n",
      "Subject-Verb agreement score (c.i): 5.0\n",
      "Verb tense usage score (c.ii): 3.5\n",
      "Sentence formation score (c.iii): 2.5\n",
      "Semantic relevance score (d.i): 3.93\n",
      "Coherence score (d.ii): 1\n",
      "\n",
      "Final score for low essay: 22.5\n",
      "Final score for high essay: 27.990000000000002\n"
     ]
    }
   ],
   "source": [
    "# 使用这个函数来计算两篇文章的最终得分\n",
    "topic = \"The best way to travel is in a group led by a tour guide\"  # 假设的评分主题\n",
    "final_score_low_essay = calculate_final_score(low_essay, topic)\n",
    "final_score_high_essay = calculate_final_score(high_essay, topic)\n",
    "\n",
    "print(f\"Final score for low essay: {final_score_low_essay}\")\n",
    "print(f\"Final score for high essay: {final_score_high_essay}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs418env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
